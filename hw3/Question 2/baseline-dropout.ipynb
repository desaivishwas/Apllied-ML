{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grade [radverma](https://github.iu.edu/CSCI-P556-Spring-2021/P556-radverma/tree/master/hw3) submission for hw3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline-Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.functional as Func\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "import os \n",
    "from os import path\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn.parallel.data_parallel as data_parallel\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.optim.lr_scheduler import LambdaLR, StepLR, MultiStepLR, ExponentialLR, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data = fetch_openml('mnist_784',version=1)\n",
    "mnist_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n",
      "0 9\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "-1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X,y=mnist_data[\"data\"],mnist_data[\"target\"]\n",
    "y=y.astype(int)\n",
    "X=((X/255.)-.5)*2\n",
    "print(X.shape,y.shape)\n",
    "print(min(y),max(y))\n",
    "print(type(X),type(y))\n",
    "print(np.min(X), np.max(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784) (60000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 784) (54000,) (6000, 784) (6000,)\n"
     ]
    }
   ],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state = 42)\n",
    "\n",
    "for train_index, val_index in split.split(X_train,y_train):\n",
    "    X_train_strat = X[train_index,:]\n",
    "    y_train_strat = y[train_index]\n",
    "    \n",
    "    X_dev_strat = X[val_index,:]\n",
    "    y_dev_strat = y[val_index]\n",
    "\n",
    "print(X_train_strat.shape,y_train_strat.shape,X_dev_strat.shape,y_dev_strat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = X_train_strat.reshape(X_train_strat.shape[0], 28, 28) #reshape the images to 54000,28,28 from 54000,784\n",
    "dev_images = X_dev_strat.reshape(X_dev_strat.shape[0], 28, 28)\n",
    "test_images = X_test.reshape(X_test.shape[0], 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "train_images_tensor = torch.tensor(train_images)/255.0\n",
    "train_labels_tensor = torch.tensor(y_train_strat)\n",
    "train_tensor = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "\n",
    "#dev\n",
    "dev_images_tensor = torch.tensor(dev_images)/255.0\n",
    "dev_labels_tensor = torch.tensor(y_dev_strat)\n",
    "dev_tensor = TensorDataset(dev_images_tensor, dev_labels_tensor)\n",
    "\n",
    "#test\n",
    "test_images_tensor = torch.tensor(test_images)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_loader = DataLoader(train_tensor, batch_size=16, num_workers=2, shuffle=True)\n",
    "# dev loader\n",
    "dev_loader = DataLoader(dev_tensor, batch_size=16, num_workers=2, shuffle=True)\n",
    "# test loader\n",
    "test_loader = DataLoader(test_images_tensor, batch_size=16, num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key hyperparameters\n",
    "class hyperparam:\n",
    "    num_hid1_units = 300\n",
    "    num_hid2_units = 100\n",
    "    num_classes = 10\n",
    "    input_dimension = 28*28\n",
    "    \n",
    "    # learning rate\n",
    "    lr = 0.05  \n",
    "    # Number of epoch\n",
    "    num_epochs = 5  \n",
    "    # Mini-batch size\n",
    "    bs = 16   \n",
    "    # Momentum SGD \n",
    "    moment= 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        )\n",
    "        \n",
    "        self.linear_block = nn.Sequential(\n",
    "            #nn.Dropout(p=0.5),\n",
    "            nn.Linear(128*7*7, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "    def forwardProp(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_block(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv_block): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (linear_block): Sequential(\n",
       "    (0): Linear(in_features=6272, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DNN and define loss function\n",
    "cnn_model = CNN()\n",
    "cnn_model.apply(initial_weights)\n",
    "cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(cnn_model.parameters(), lr=hyperparam.lr, momentum=hyperparam.moment)\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if torch.cuda.is_available():\n",
    "    print('Yes, Available')\n",
    "    cnn_model = cnn_model.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishw\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-8754cc049bf9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m#Model computations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mout1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;31m#Cross Entropy loss calculation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \"\"\"\n\u001b[1;32m--> 201\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train the DNN\n",
    "train_avg_loss= []\n",
    "train_accuracy =[]\n",
    "dev_avgLoss = []\n",
    "dev_accuracy= []\n",
    "\n",
    "#loop over epochs\n",
    "for epoch in range(hyperparam.num_epochs):\n",
    "    tr_num_correct = 0\n",
    "    tr_num_samples = 0\n",
    "    tr_total_loss = 0.0\n",
    "    dev_num_correct = 0\n",
    "    dev_num_samples = 0\n",
    "    dev_total_loss = 0.0\n",
    "    \n",
    "    #Training\n",
    "    cnn_model.train(True) #does each opoch initialise the entire dnn model with all the weights and stuff?\n",
    "    scheduler.step()\n",
    "\n",
    "    with torch.set_grad_enabled(True):\n",
    "        for local_batch, local_labels in train_loader:\n",
    "            local_batch=local_batch.unsqueeze(1)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "              local_batch = local_batch.cuda()\n",
    "              local_labels = local_labels.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            local_batch = local_batch.float()\n",
    "            local_labels = local_labels.float()\n",
    "            #local_batch, local_labels = Variable(local_batch), Variable(local_labels)\n",
    "            \n",
    "            #Model computations\n",
    "            out1 = cnn_model(local_batch)\n",
    "            \n",
    "            #Cross Entropy loss calculation\n",
    "            ploss = criterion(out1, local_labels.long())\n",
    "            tr_total_loss += ploss*hyperparam.bs\n",
    "            \n",
    "            #Backpropogation\n",
    "            ploss.backward() #gradient calculation\n",
    "            optimizer.step() #weight update\n",
    "            #scheduler.step() #to decrease the lr by 10% every 10 epochs because as you move towards the minima the steps size needs to decrease so it doesnt jump over the minima\n",
    "            \n",
    "            sel_class = torch.argmax(out1, dim=1) #chooses the maximum value of the softmax layer\n",
    "            \n",
    "            tr_num_correct += sel_class.eq(local_labels).sum().item()\n",
    "            tr_num_samples += hyperparam.bs\n",
    "        \n",
    "        tr_avg_Loss = tr_total_loss/len(train_loader.dataset)\n",
    "        train_avg_loss.append(tr_avgLoss)\n",
    "        \n",
    "        tr_accuracy = tr_num_correct/tr_num_samples\n",
    "        train_accuracy.append(tr_accuracy)\n",
    "\n",
    "    #validation \n",
    "    with torch.set_grad_enabled(False):\n",
    "      cnn_model.eval()\n",
    "\n",
    "      for local_batch,local_labels in dev_loader:\n",
    "        local_batch=local_batch.unsqueeze(1)\n",
    "\n",
    "        local_batch=local_batch.float()\n",
    "        local_labels=local_labels.float()\n",
    "        \n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            local_batch = local_batch.cuda()\n",
    "            local_labels = local_labels.cuda()\n",
    "     \n",
    "        out1=cnn_model(local_batch)\n",
    "\n",
    "        #CrossEntropy \n",
    "        pLoss=criterion(out1,local_labels.long())\n",
    "        dev_total_loss +=ploss*hyperparam.bs\n",
    "        sel_class=torch.argmax(out1,dim=1)\n",
    "\n",
    "        dev_num_correct+=sel_class.eq(local_labels).sum().item()\n",
    "\n",
    "        #print(correction)\n",
    "        dev_num_samples+=hyperparam.bs\n",
    "\n",
    "      dev_avgLoss=dev_total_loss/len(dev_loader.dataset)\n",
    "      dev_avgLoss.append(dev_avgLoss)\n",
    "\n",
    "      dev_accuracy=dev_num_correct/dev_num_samples\n",
    "      dev_accuracy.append(dev_accuracy)\n",
    "    print(\"Accuracy\",tr_accuracy,dev_accuracy)\n",
    "\n",
    "# Train Accuracy\n",
    "plt.plot(np.arange(0,hyperparam.num_epochs,1),tr_accuracy_list)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Train Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "# Validation\n",
    "plt.plot(np.arange(0,hyperparam.num_epochs,1),dev_accuracy_list)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "# ------ Errors ---------\n",
    "\n",
    "# Train Errors\n",
    "plt.plot(np.arange(0,hyperparam.num_epochs,1),tr_avgLoss_list)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Train Errors\")\n",
    "plt.show()\n",
    "\n",
    "# Errors\n",
    "plt.plot(np.arange(0,hyperparam.num_epochs,1),dev_avgLoss_list)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Errors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(data_loader):\n",
    "    cnn_model.eval()\n",
    "    test_predictions = torch.LongTensor()\n",
    "    \n",
    "    for i, data in enumerate(data_loader):\n",
    "        data = data.unsqueeze(1) \n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            \n",
    "        output = cnn_model(data.float()) \n",
    "        preds = output.cpu().data.max(1, keepdim=True)[1]\n",
    "        test_preds = torch.cat((test_preds, preds), dim=0)\n",
    "        \n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_preds = make_predictions(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df=pd.DataFrame()\n",
    "submission_df[\"label\"]=test_set_preds.numpy().squeeze()\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = mnist[\"target\"][60000:].astype(int)\n",
    "y_predictions = np.array(submission_df[\"label\"])\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
