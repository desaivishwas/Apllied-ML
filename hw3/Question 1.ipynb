{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Question 1\n\n#### Grade [radverma](https://github.iu.edu/CSCI-P556-Spring-2021/P556-radverma/tree/master/hw3) submission for hw3","metadata":{}},{"cell_type":"code","source":"import math\nimport numpy as np\nfrom copy import deepcopy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Implementation of sigmoid function and derivative of a sigmoid function. Sigmoid function will be used for all activation units. Derivative will be used to calculate weight updates.","metadata":{}},{"cell_type":"code","source":"def init_weights():\n    return [np.random.uniform(-1, 1, (5, 4))] + [np.random.uniform(-1, 1, (5, 1))]\n\n\ndef to_binary(n):\n    return np.unpackbits(np.arange(n, n + 1, dtype='>i%d' % 1).view(np.uint8))[-4:]\n\n\n# ACTIVATION FUNCTION\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n    # return math.tanh(x)\n\n\ndef train():\n    X = np.array([[1] + list(to_binary(i)) for i in range(0, 16)]).T\n    D = np.array([bin(i).count(\"1\") % 2 != 0 for i in range(0, 16)]).reshape(1, 16).astype(int)\n    return X, D","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Implementation of forward pass\n\n#### Implementation of backward pass with momentum, where momentum is zero, when momentum is not to be used.","metadata":{}},{"cell_type":"code","source":"def forward_propagation(x, layers):\n    output = []\n    output.append(sigmoid(np.dot(layers[0].T, x)))\n    output.append(np.dot(layers[0].T, np.insert(output[0], 0, 1, axis=0)))\n    return np.array(output)\n\n\ndef backward_propagation(x, Y, d1, L, eta, delta_l, alpha):\n    y = Y[1]\n    deltaK = y * (1 - y) * (d1 - y)\n    delta_l[1] = eta * deltaK.T * np.insert(Y[0], 0, 1, axis=0) + (alpha * delta_l[1])\n\n    deltaJ = Y[0] * (1 - Y[0]) * L[1][1:, :] * deltaK\n    delta_l[0] = eta * deltaJ.T * x + (alpha * delta_l[0])\n\n    L[0] += delta_l[0]\n    L[1] = L[1] + delta_l[1]\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Multi Layer Perceptron","metadata":{}},{"cell_type":"code","source":"def MultiLayerPerceptron(X, D, L, eta, alpha):\n    epoch = 0\n    delta_l = [np.zeros((5, 4))] + [np.zeros((5, 1))]\n\n    while True:\n        error = False\n        rng = np.random.default_rng()\n        stochasticX = rng.permutation(X)\n        for i in range(0, 16):\n            x = stochasticX[:, i].reshape(5, 1)\n            Y = forward_propagation(x, L)\n            backward_propagation(x, Y, np.reshape(D[:, i], (1, 1)), L, eta, delta_l, alpha)\n\n            if np.abs(np.squeeze(D[:, i] - Y[1])) < 0.05:\n                error = True\n            if epoch % 10000 == 0:\n                print(\"Epoch\", epoch)\n        epoch += 1\n\n        if not error:\n            break\n\n    return epoch\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    X, D = train()\n    L = init_weights()\n    for eta in np.arange(0.05, 0.51, 0.05):\n        print(\"eta : \", eta, \", momentum : \", 0, \" epoch : \", MultiLayerPerceptron(X, D, deepcopy(L), eta, 0))\n        print(\"eta : \", eta, \", momentum :\", 0.9, \" epoch : \", MultiLayerPerceptron(X, D, deepcopy(L), eta, 0.9))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}